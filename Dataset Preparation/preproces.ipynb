{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the original dataset with the tensor obtained after peek detection to find the intervall where all seasonality are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3356/819635723.py:1: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  series = pd.read_csv('../dataset/ARPA/ARPA.csv', header=0, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_3356/819635723.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  tmp_row[i] = row[i]\n"
     ]
    }
   ],
   "source": [
    "series = pd.read_csv('../dataset/ARPA/ARPA.csv', header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "column_names = series.columns.to_list()\n",
    "time_index = series.index\n",
    "wavelet_result = torch.load(\"../Multivariate/result.pt\")\n",
    "\n",
    "def train_test_split(lst, percentage=80.0):\n",
    "    # Calcola il numero di elementi da prendere\n",
    "    num_elements = int(len(lst) * (percentage / 100.0))\n",
    "    # Prendi il numero desiderato di elementi\n",
    "    return lst[:num_elements]\n",
    "\n",
    "rows_to_add = []\n",
    "\n",
    "if len(column_names) > 1:\n",
    "  # Iter through each row of the DataFrame\n",
    "  for index, row in series.iterrows():\n",
    "    tmp_row = [0] * len(row)\n",
    "    row_index = series.index.get_loc(index)\n",
    "\n",
    "    for i in range(0, len(column_names)):\n",
    "      if torch.any(torch.eq(torch.tensor(1), wavelet_result[row_index, i])):\n",
    "        tmp_row[i] = row[i]\n",
    "      # if torch.sum(torch.eq(wavelet_result[row_index, i], 1)) >= 2:\n",
    "      #   tmp_row[i] = row[i]\n",
    "      else:\n",
    "        tmp_row[i] = np.nan\n",
    "    rows_to_add.append(tmp_row)\n",
    "else:\n",
    "  for index, row in series.iterrows():\n",
    "    tmp_row = [0] * len(row)\n",
    "    row_index = series.index.get_loc(index)\n",
    "    if torch.any(torch.eq(torch.tensor(1), wavelet_result[row_index])):\n",
    "      tmp_row = row.values[0]\n",
    "    # if torch.sum(torch.eq(wavelet_result[row_index], 1)) >= 2:\n",
    "    #   tmp_row = row.values[0]\n",
    "    else:\n",
    "      tmp_row = np.nan\n",
    "    rows_to_add.append(tmp_row)\n",
    "\n",
    "# Used to select the percentage of the dataset to be used\n",
    "\n",
    "# rows_to_add = train_test_split(rows_to_add)\n",
    "# time_index = train_test_split(time_index)\n",
    "\n",
    "# Create a DataFrame for the coefficients of the current dimension\n",
    "final_df = pd.DataFrame(data=rows_to_add, index=time_index, columns=[f'{name}' for name in column_names])\n",
    "\n",
    "final_df.to_csv('final_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try move the series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is to move data to obtain a new series with the removed part, due to seasonality match found only in small fraction data are selected based on a single match instead of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def first_index(data):\n",
    "    # Trova gli indici dei valori non NaN nel dataset\n",
    "    indices = np.where(~np.isnan(data))[0]\n",
    "    \n",
    "    # Se non ci sono valori non NaN, restituisci -1\n",
    "    if len(indices) == 0:\n",
    "        return -1\n",
    "    \n",
    "    # Calcola le differenze tra gli indici per trovare i blocchi consecutivi\n",
    "    diff_indices = np.diff(indices)\n",
    "    \n",
    "    # Trova il blocco più lungo di indici consecutivi\n",
    "    longest_block_index = np.argmax(np.bincount(diff_indices))\n",
    "    \n",
    "    # Trova il primo indice di questo blocco nel dataset originale\n",
    "    first_index_longest_block = indices[np.where(diff_indices == longest_block_index)[0][0]]\n",
    "    \n",
    "    return first_index_longest_block\n",
    "\n",
    "series = pd.read_csv('final_dataset.csv', header=0, index_col=0, parse_dates=True)\n",
    "time_index = series.index\n",
    "\n",
    "\n",
    "for column in series.columns:\n",
    "    col = series[column].values\n",
    "    max_index = first_index(col)\n",
    "    print(max_index)\n",
    "    count = sum(1 for j in range(0, max_index) if not np.isnan(col[j]))\n",
    "    data_without_nan = [x for x in col if not np.isnan(x)]\n",
    "    start = max_index-count\n",
    "    end = len(data_without_nan)-start\n",
    "    time = time_index[start:end]\n",
    "    final_df = pd.DataFrame(data = data_without_nan, index=time, columns = [column])\n",
    "    final_df.to_csv(f'{column}.csv')   \n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame for the coefficients of the current dimension\n",
    "#final_df = pd.DataFrame(data=data_without_nan, index=time_index, columns=[f'{name}' for name in column_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second try use the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "80\n",
      "80\n",
      "0\n",
      "85\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "def first_index(data):\n",
    "    # Trova gli indici dei valori non NaN nel dataset\n",
    "    indices = np.where(~np.isnan(data))[0]\n",
    "    \n",
    "    # Se non ci sono valori non NaN, restituisci -1\n",
    "    if len(indices) == 0:\n",
    "        return -1\n",
    "    \n",
    "    # Calcola le differenze tra gli indici per trovare i blocchi consecutivi\n",
    "    diff_indices = np.diff(indices)\n",
    "    \n",
    "    # Trova il blocco più lungo di indici consecutivi\n",
    "    longest_block_index = np.argmax(np.bincount(diff_indices))\n",
    "    \n",
    "    # Trova il primo indice di questo blocco nel dataset originale\n",
    "    first_index_longest_block = indices[np.where(diff_indices == longest_block_index)[0][0]]\n",
    "    \n",
    "    return first_index_longest_block\n",
    "\n",
    "series = pd.read_csv('final_dataset.csv', header=0, index_col=0, parse_dates=True)\n",
    "time_index = series.index\n",
    "\n",
    "for column in series.columns:\n",
    "    col = series[column].values\n",
    "    \n",
    "    max_index = first_index(col)\n",
    "    count = 0\n",
    "    start = max_index\n",
    "    while not np.isnan(col[max_index]) and max_index < len(col)-1:\n",
    "        count += 1\n",
    "        max_index += 1\n",
    "        \n",
    "\n",
    "    print(start)\n",
    "    print(count)\n",
    "    end = start + count\n",
    "    print(end)\n",
    "    time = time_index[start:end]\n",
    "    final_df = pd.DataFrame(data = col[start:end], index=time, columns = [column])\n",
    "    final_df.to_csv(f'{column}.csv')   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
